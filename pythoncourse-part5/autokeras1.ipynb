{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Convolutional Neural Network models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. CNN classifiers for identifying handwritten digits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    return mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3. CNN classifier for identifying other objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CIFAR100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dataset of 50,000 32x32 color training images => 32x32x3, \n",
    "# labeled over 100 categories, and 10,000 test images.\n",
    "import autokeras as ak\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.datasets import cifar100\n",
    "\n",
    "def to_one_hot(y):\n",
    "    y_encoder = OneHotEncoder()\n",
    "    y_encoder.fit(y)\n",
    "    y = y_encoder.transform(y)\n",
    "    return y, y_encoder\n",
    "\n",
    "(x_train, y_train), (x_test, y_test) = cifar100.load_data()\n",
    "\n",
    "_, canvas = plt.subplots(1, 2)\n",
    "_ = canvas[0].imshow(x_train[1234])\n",
    "_ = canvas[1].imshow(x_test[1234])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train for 1250 steps, validate for 313 steps\n",
      "Epoch 1/10\n",
      "1250/1250 [==============================] - ETA: 17:54 - loss: 4.6414 - accuracy: 0.031 - ETA: 2:06 - loss: 4.8775 - accuracy: 0.017 - ETA: 1:10 - loss: 4.7521 - accuracy: 0.01 - ETA: 48s - loss: 4.6769 - accuracy: 0.0180 - ETA: 38s - loss: 4.6277 - accuracy: 0.023 - ETA: 32s - loss: 4.5945 - accuracy: 0.025 - ETA: 27s - loss: 4.5579 - accuracy: 0.028 - ETA: 24s - loss: 4.5426 - accuracy: 0.029 - ETA: 22s - loss: 4.5113 - accuracy: 0.035 - ETA: 20s - loss: 4.4677 - accuracy: 0.041 - ETA: 18s - loss: 4.4460 - accuracy: 0.044 - ETA: 17s - loss: 4.4078 - accuracy: 0.047 - ETA: 17s - loss: 4.3976 - accuracy: 0.049 - ETA: 16s - loss: 4.3831 - accuracy: 0.050 - ETA: 15s - loss: 4.3662 - accuracy: 0.050 - ETA: 15s - loss: 4.3455 - accuracy: 0.053 - ETA: 14s - loss: 4.3224 - accuracy: 0.057 - ETA: 14s - loss: 4.3035 - accuracy: 0.059 - ETA: 13s - loss: 4.2927 - accuracy: 0.060 - ETA: 13s - loss: 4.2766 - accuracy: 0.062 - ETA: 13s - loss: 4.2687 - accuracy: 0.062 - ETA: 13s - loss: 4.2544 - accuracy: 0.064 - ETA: 12s - loss: 4.2453 - accuracy: 0.066 - ETA: 12s - loss: 4.2321 - accuracy: 0.067 - ETA: 12s - loss: 4.2176 - accuracy: 0.069 - ETA: 11s - loss: 4.2008 - accuracy: 0.072 - ETA: 11s - loss: 4.1912 - accuracy: 0.073 - ETA: 11s - loss: 4.1835 - accuracy: 0.075 - ETA: 11s - loss: 4.1733 - accuracy: 0.076 - ETA: 11s - loss: 4.1553 - accuracy: 0.079 - ETA: 11s - loss: 4.1435 - accuracy: 0.081 - ETA: 10s - loss: 4.1315 - accuracy: 0.084 - ETA: 10s - loss: 4.1176 - accuracy: 0.087 - ETA: 10s - loss: 4.1042 - accuracy: 0.089 - ETA: 10s - loss: 4.0902 - accuracy: 0.091 - ETA: 9s - loss: 4.0802 - accuracy: 0.093 - ETA: 9s - loss: 4.0677 - accuracy: 0.09 - ETA: 9s - loss: 4.0559 - accuracy: 0.09 - ETA: 9s - loss: 4.0452 - accuracy: 0.09 - ETA: 8s - loss: 4.0372 - accuracy: 0.10 - ETA: 8s - loss: 4.0260 - accuracy: 0.10 - ETA: 8s - loss: 4.0145 - accuracy: 0.10 - ETA: 8s - loss: 4.0096 - accuracy: 0.10 - ETA: 8s - loss: 4.0031 - accuracy: 0.10 - ETA: 8s - loss: 3.9910 - accuracy: 0.10 - ETA: 7s - loss: 3.9824 - accuracy: 0.10 - ETA: 7s - loss: 3.9725 - accuracy: 0.10 - ETA: 7s - loss: 3.9656 - accuracy: 0.11 - ETA: 7s - loss: 3.9611 - accuracy: 0.11 - ETA: 7s - loss: 3.9500 - accuracy: 0.11 - ETA: 7s - loss: 3.9428 - accuracy: 0.11 - ETA: 7s - loss: 3.9379 - accuracy: 0.11 - ETA: 7s - loss: 3.9360 - accuracy: 0.11 - ETA: 7s - loss: 3.9288 - accuracy: 0.11 - ETA: 7s - loss: 3.9202 - accuracy: 0.11 - ETA: 6s - loss: 3.9125 - accuracy: 0.12 - ETA: 6s - loss: 3.9041 - accuracy: 0.12 - ETA: 6s - loss: 3.9002 - accuracy: 0.12 - ETA: 6s - loss: 3.8951 - accuracy: 0.12 - ETA: 6s - loss: 3.8907 - accuracy: 0.12 - ETA: 6s - loss: 3.8868 - accuracy: 0.12 - ETA: 6s - loss: 3.8809 - accuracy: 0.12 - ETA: 6s - loss: 3.8752 - accuracy: 0.12 - ETA: 6s - loss: 3.8696 - accuracy: 0.12 - ETA: 5s - loss: 3.8634 - accuracy: 0.12 - ETA: 5s - loss: 3.8597 - accuracy: 0.12 - ETA: 5s - loss: 3.8544 - accuracy: 0.13 - ETA: 5s - loss: 3.8451 - accuracy: 0.13 - ETA: 5s - loss: 3.8406 - accuracy: 0.13 - ETA: 5s - loss: 3.8348 - accuracy: 0.13 - ETA: 5s - loss: 3.8313 - accuracy: 0.13 - ETA: 5s - loss: 3.8269 - accuracy: 0.13 - ETA: 5s - loss: 3.8211 - accuracy: 0.13 - ETA: 5s - loss: 3.8149 - accuracy: 0.13 - ETA: 5s - loss: 3.8122 - accuracy: 0.13 - ETA: 4s - loss: 3.8082 - accuracy: 0.13 - ETA: 4s - loss: 3.8002 - accuracy: 0.13 - ETA: 4s - loss: 3.7957 - accuracy: 0.13 - ETA: 4s - loss: 3.7884 - accuracy: 0.14 - ETA: 4s - loss: 3.7829 - accuracy: 0.14 - ETA: 4s - loss: 3.7783 - accuracy: 0.14 - ETA: 4s - loss: 3.7735 - accuracy: 0.14 - ETA: 4s - loss: 3.7707 - accuracy: 0.14 - ETA: 4s - loss: 3.7670 - accuracy: 0.14 - ETA: 4s - loss: 3.7632 - accuracy: 0.14 - ETA: 4s - loss: 3.7588 - accuracy: 0.14 - ETA: 4s - loss: 3.7569 - accuracy: 0.14 - ETA: 3s - loss: 3.7527 - accuracy: 0.14 - ETA: 3s - loss: 3.7507 - accuracy: 0.14 - ETA: 3s - loss: 3.7462 - accuracy: 0.14 - ETA: 3s - loss: 3.7418 - accuracy: 0.14 - ETA: 3s - loss: 3.7380 - accuracy: 0.14 - ETA: 3s - loss: 3.7329 - accuracy: 0.15 - ETA: 3s - loss: 3.7267 - accuracy: 0.15 - ETA: 3s - loss: 3.7203 - accuracy: 0.15 - ETA: 3s - loss: 3.7154 - accuracy: 0.15 - ETA: 3s - loss: 3.7095 - accuracy: 0.15 - ETA: 3s - loss: 3.7044 - accuracy: 0.15 - ETA: 3s - loss: 3.7001 - accuracy: 0.15 - ETA: 2s - loss: 3.6946 - accuracy: 0.15 - ETA: 2s - loss: 3.6893 - accuracy: 0.15 - ETA: 2s - loss: 3.6838 - accuracy: 0.15 - ETA: 2s - loss: 3.6782 - accuracy: 0.16 - ETA: 2s - loss: 3.6721 - accuracy: 0.16 - ETA: 2s - loss: 3.6695 - accuracy: 0.16 - ETA: 2s - loss: 3.6668 - accuracy: 0.16 - ETA: 2s - loss: 3.6646 - accuracy: 0.16 - ETA: 2s - loss: 3.6611 - accuracy: 0.16 - ETA: 2s - loss: 3.6562 - accuracy: 0.16 - ETA: 2s - loss: 3.6526 - accuracy: 0.16 - ETA: 2s - loss: 3.6489 - accuracy: 0.16 - ETA: 2s - loss: 3.6446 - accuracy: 0.16 - ETA: 2s - loss: 3.6412 - accuracy: 0.16 - ETA: 1s - loss: 3.6377 - accuracy: 0.16 - ETA: 1s - loss: 3.6328 - accuracy: 0.16 - ETA: 1s - loss: 3.6278 - accuracy: 0.16 - ETA: 1s - loss: 3.6243 - accuracy: 0.16 - ETA: 1s - loss: 3.6201 - accuracy: 0.17 - ETA: 1s - loss: 3.6167 - accuracy: 0.17 - ETA: 1s - loss: 3.6138 - accuracy: 0.17 - ETA: 1s - loss: 3.6108 - accuracy: 0.17 - ETA: 1s - loss: 3.6062 - accuracy: 0.17 - ETA: 1s - loss: 3.6047 - accuracy: 0.17 - ETA: 1s - loss: 3.6036 - accuracy: 0.17 - ETA: 1s - loss: 3.6011 - accuracy: 0.17 - ETA: 1s - loss: 3.5987 - accuracy: 0.17 - ETA: 1s - loss: 3.5952 - accuracy: 0.17 - ETA: 1s - loss: 3.5922 - accuracy: 0.17 - ETA: 1s - loss: 3.5918 - accuracy: 0.17 - ETA: 1s - loss: 3.5878 - accuracy: 0.17 - ETA: 0s - loss: 3.5843 - accuracy: 0.17 - ETA: 0s - loss: 3.5810 - accuracy: 0.17 - ETA: 0s - loss: 3.5773 - accuracy: 0.17 - ETA: 0s - loss: 3.5740 - accuracy: 0.17 - ETA: 0s - loss: 3.5714 - accuracy: 0.17 - ETA: 0s - loss: 3.5690 - accuracy: 0.17 - ETA: 0s - loss: 3.5664 - accuracy: 0.17 - ETA: 0s - loss: 3.5629 - accuracy: 0.17 - ETA: 0s - loss: 3.5600 - accuracy: 0.18 - ETA: 0s - loss: 3.5556 - accuracy: 0.18 - ETA: 0s - loss: 3.5533 - accuracy: 0.18 - ETA: 0s - loss: 3.5510 - accuracy: 0.18 - ETA: 0s - loss: 3.5482 - accuracy: 0.18 - ETA: 0s - loss: 3.5458 - accuracy: 0.18 - ETA: 0s - loss: 3.5434 - accuracy: 0.18 - 10s 8ms/step - loss: 3.5416 - accuracy: 0.1836 - val_loss: 3.0304 - val_accuracy: 0.2843\n",
      "Epoch 2/10\n",
      "1238/1250 [============================>.] - ETA: 11:18 - loss: 3.3330 - accuracy: 0.187 - ETA: 1:22 - loss: 2.9366 - accuracy: 0.312 - ETA: 44s - loss: 3.0070 - accuracy: 0.2847 - ETA: 29s - loss: 3.0276 - accuracy: 0.296 - ETA: 22s - loss: 2.9814 - accuracy: 0.291 - ETA: 19s - loss: 2.9918 - accuracy: 0.288 - ETA: 16s - loss: 2.9895 - accuracy: 0.286 - ETA: 15s - loss: 2.9810 - accuracy: 0.286 - ETA: 14s - loss: 2.9711 - accuracy: 0.287 - ETA: 13s - loss: 2.9494 - accuracy: 0.286 - ETA: 12s - loss: 2.9447 - accuracy: 0.290 - ETA: 11s - loss: 2.9445 - accuracy: 0.288 - ETA: 11s - loss: 2.9510 - accuracy: 0.288 - ETA: 11s - loss: 2.9533 - accuracy: 0.287 - ETA: 12s - loss: 2.9489 - accuracy: 0.288 - ETA: 12s - loss: 2.9409 - accuracy: 0.291 - ETA: 11s - loss: 2.9419 - accuracy: 0.291 - ETA: 11s - loss: 2.9440 - accuracy: 0.290 - ETA: 10s - loss: 2.9355 - accuracy: 0.290 - ETA: 10s - loss: 2.9451 - accuracy: 0.288 - ETA: 10s - loss: 2.9449 - accuracy: 0.289 - ETA: 9s - loss: 2.9362 - accuracy: 0.291 - ETA: 9s - loss: 2.9324 - accuracy: 0.29 - ETA: 9s - loss: 2.9290 - accuracy: 0.29 - ETA: 9s - loss: 2.9196 - accuracy: 0.29 - ETA: 8s - loss: 2.9116 - accuracy: 0.29 - ETA: 8s - loss: 2.9171 - accuracy: 0.29 - ETA: 8s - loss: 2.9129 - accuracy: 0.29 - ETA: 8s - loss: 2.9092 - accuracy: 0.29 - ETA: 8s - loss: 2.9067 - accuracy: 0.29 - ETA: 7s - loss: 2.9021 - accuracy: 0.29 - ETA: 7s - loss: 2.9018 - accuracy: 0.29 - ETA: 7s - loss: 2.9003 - accuracy: 0.29 - ETA: 7s - loss: 2.8980 - accuracy: 0.29 - ETA: 7s - loss: 2.8987 - accuracy: 0.29 - ETA: 7s - loss: 2.8953 - accuracy: 0.29 - ETA: 7s - loss: 2.8869 - accuracy: 0.29 - ETA: 7s - loss: 2.8851 - accuracy: 0.29 - ETA: 7s - loss: 2.8862 - accuracy: 0.29 - ETA: 6s - loss: 2.8837 - accuracy: 0.29 - ETA: 6s - loss: 2.8806 - accuracy: 0.29 - ETA: 6s - loss: 2.8771 - accuracy: 0.30 - ETA: 6s - loss: 2.8783 - accuracy: 0.30 - ETA: 6s - loss: 2.8780 - accuracy: 0.30 - ETA: 6s - loss: 2.8740 - accuracy: 0.30 - ETA: 6s - loss: 2.8705 - accuracy: 0.30 - ETA: 6s - loss: 2.8691 - accuracy: 0.30 - ETA: 5s - loss: 2.8710 - accuracy: 0.30 - ETA: 5s - loss: 2.8693 - accuracy: 0.30 - ETA: 5s - loss: 2.8694 - accuracy: 0.30 - ETA: 5s - loss: 2.8667 - accuracy: 0.30 - ETA: 5s - loss: 2.8676 - accuracy: 0.30 - ETA: 5s - loss: 2.8644 - accuracy: 0.30 - ETA: 5s - loss: 2.8673 - accuracy: 0.30 - ETA: 5s - loss: 2.8656 - accuracy: 0.30 - ETA: 5s - loss: 2.8662 - accuracy: 0.30 - ETA: 5s - loss: 2.8605 - accuracy: 0.30 - ETA: 5s - loss: 2.8590 - accuracy: 0.30 - ETA: 4s - loss: 2.8580 - accuracy: 0.30 - ETA: 4s - loss: 2.8563 - accuracy: 0.30 - ETA: 4s - loss: 2.8534 - accuracy: 0.30 - ETA: 4s - loss: 2.8557 - accuracy: 0.30 - ETA: 4s - loss: 2.8547 - accuracy: 0.30 - ETA: 4s - loss: 2.8554 - accuracy: 0.30 - ETA: 4s - loss: 2.8533 - accuracy: 0.30 - ETA: 4s - loss: 2.8521 - accuracy: 0.30 - ETA: 4s - loss: 2.8536 - accuracy: 0.30 - ETA: 4s - loss: 2.8549 - accuracy: 0.30 - ETA: 4s - loss: 2.8529 - accuracy: 0.30 - ETA: 4s - loss: 2.8511 - accuracy: 0.30 - ETA: 4s - loss: 2.8507 - accuracy: 0.30 - ETA: 3s - loss: 2.8488 - accuracy: 0.30 - ETA: 3s - loss: 2.8484 - accuracy: 0.30 - ETA: 3s - loss: 2.8474 - accuracy: 0.30 - ETA: 3s - loss: 2.8453 - accuracy: 0.30 - ETA: 3s - loss: 2.8456 - accuracy: 0.30 - ETA: 3s - loss: 2.8470 - accuracy: 0.30 - ETA: 3s - loss: 2.8459 - accuracy: 0.30 - ETA: 3s - loss: 2.8452 - accuracy: 0.30 - ETA: 3s - loss: 2.8464 - accuracy: 0.30 - ETA: 3s - loss: 2.8462 - accuracy: 0.30 - ETA: 3s - loss: 2.8476 - accuracy: 0.30 - ETA: 3s - loss: 2.8484 - accuracy: 0.30 - ETA: 3s - loss: 2.8479 - accuracy: 0.30 - ETA: 3s - loss: 2.8497 - accuracy: 0.30 - ETA: 3s - loss: 2.8488 - accuracy: 0.30 - ETA: 2s - loss: 2.8447 - accuracy: 0.30 - ETA: 2s - loss: 2.8434 - accuracy: 0.30 - ETA: 2s - loss: 2.8411 - accuracy: 0.30 - ETA: 2s - loss: 2.8397 - accuracy: 0.30 - ETA: 2s - loss: 2.8404 - accuracy: 0.30 - ETA: 2s - loss: 2.8370 - accuracy: 0.30 - ETA: 2s - loss: 2.8345 - accuracy: 0.30 - ETA: 2s - loss: 2.8314 - accuracy: 0.30 - ETA: 2s - loss: 2.8292 - accuracy: 0.31 - ETA: 2s - loss: 2.8277 - accuracy: 0.31 - ETA: 2s - loss: 2.8244 - accuracy: 0.31 - ETA: 2s - loss: 2.8239 - accuracy: 0.31 - ETA: 2s - loss: 2.8234 - accuracy: 0.31 - ETA: 2s - loss: 2.8251 - accuracy: 0.31 - ETA: 2s - loss: 2.8242 - accuracy: 0.31 - ETA: 2s - loss: 2.8217 - accuracy: 0.31 - ETA: 2s - loss: 2.8201 - accuracy: 0.31 - ETA: 1s - loss: 2.8192 - accuracy: 0.31 - ETA: 1s - loss: 2.8181 - accuracy: 0.31 - ETA: 1s - loss: 2.8175 - accuracy: 0.31 - ETA: 1s - loss: 2.8176 - accuracy: 0.31 - ETA: 1s - loss: 2.8172 - accuracy: 0.31 - ETA: 1s - loss: 2.8151 - accuracy: 0.31 - ETA: 1s - loss: 2.8119 - accuracy: 0.31 - ETA: 1s - loss: 2.8125 - accuracy: 0.31 - ETA: 1s - loss: 2.8114 - accuracy: 0.31 - ETA: 1s - loss: 2.8103 - accuracy: 0.31 - ETA: 1s - loss: 2.8100 - accuracy: 0.31 - ETA: 1s - loss: 2.8094 - accuracy: 0.31 - ETA: 1s - loss: 2.8093 - accuracy: 0.31 - ETA: 1s - loss: 2.8115 - accuracy: 0.31 - ETA: 1s - loss: 2.8119 - accuracy: 0.31 - ETA: 1s - loss: 2.8100 - accuracy: 0.31 - ETA: 1s - loss: 2.8088 - accuracy: 0.31 - ETA: 0s - loss: 2.8084 - accuracy: 0.31 - ETA: 0s - loss: 2.8068 - accuracy: 0.31 - ETA: 0s - loss: 2.8063 - accuracy: 0.31 - ETA: 0s - loss: 2.8047 - accuracy: 0.31 - ETA: 0s - loss: 2.8038 - accuracy: 0.31 - ETA: 0s - loss: 2.8028 - accuracy: 0.31 - ETA: 0s - loss: 2.8028 - accuracy: 0.31 - ETA: 0s - loss: 2.8023 - accuracy: 0.31 - ETA: 0s - loss: 2.8011 - accuracy: 0.31 - ETA: 0s - loss: 2.8000 - accuracy: 0.31 - ETA: 0s - loss: 2.7991 - accuracy: 0.31 - ETA: 0s - loss: 2.7994 - accuracy: 0.31 - ETA: 0s - loss: 2.7991 - accuracy: 0.31 - ETA: 0s - loss: 2.7990 - accuracy: 0.31 - ETA: 0s - loss: 2.7982 - accuracy: 0.3161WARNING:tensorflow:Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: loss,accuracy\n",
      "WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-23-d443d2e39e73>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#             time_limit=20*60)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mclf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mak\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mImageClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmax_trials\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mclf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\autokeras\\tasks\\image.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, epochs, callbacks, validation_split, validation_data, **kwargs)\u001b[0m\n\u001b[0;32m    120\u001b[0m                     \u001b[0mvalidation_split\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    121\u001b[0m                     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 122\u001b[1;33m                     **kwargs)\n\u001b[0m\u001b[0;32m    123\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    124\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\autokeras\\auto_model.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, callbacks, validation_split, validation_data, **kwargs)\u001b[0m\n\u001b[0;32m    256\u001b[0m                           \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    257\u001b[0m                           \u001b[0mfit_on_val_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_split_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 258\u001b[1;33m                           **kwargs)\n\u001b[0m\u001b[0;32m    259\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    260\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_process_x\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\autokeras\\engine\\tuner.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, callbacks, fit_on_val_data, **fit_kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m             \u001b[0mnew_callbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mEarlyStopping\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatience\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 114\u001b[1;33m         \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnew_callbacks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    115\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    116\u001b[0m         \u001b[1;31m# Fully train the best model with original callbacks.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\kerastuner\\engine\\base_tuner.py\u001b[0m in \u001b[0;36msearch\u001b[1;34m(self, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m    128\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    129\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_trial_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 130\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_trial\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    131\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_trial_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    132\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_search_end\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\autokeras\\engine\\tuner.py\u001b[0m in \u001b[0;36mrun_trial\u001b[1;34m(self, trial, x, *fit_args, **fit_kwargs)\u001b[0m\n\u001b[0;32m     69\u001b[0m             \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhypermodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbuild\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhyperparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m             \u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madapt_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0mfit_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mcopied_fit_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mmetric\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_values\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     73\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moracle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirection\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'min'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    817\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    818\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 819\u001b[1;33m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[0;32m    820\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m   def evaluate(self,\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, model, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m    340\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    341\u001b[0m                 \u001b[0mtraining_context\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 342\u001b[1;33m                 total_epochs=epochs)\n\u001b[0m\u001b[0;32m    343\u001b[0m             \u001b[0mcbks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    344\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[1;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[0;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[0;32m    127\u001b[0m       \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m         \u001b[0mbatch_outs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    129\u001b[0m       \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m         \u001b[1;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\engine\\training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[1;34m(input_fn)\u001b[0m\n\u001b[0;32m     96\u001b[0m     \u001b[1;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[1;32m---> 98\u001b[1;33m                               distributed_function(input_fn))\n\u001b[0m\u001b[0;32m     99\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    100\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    567\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 568\u001b[1;33m       \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    569\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    570\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    597\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 599\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    600\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    601\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2363\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2364\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2365\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1611\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1612\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1613\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#y_train, y_encoder = to_one_hot(y_train)\n",
    "#y_test, _ = to_one_hot(y_test)\n",
    "#train_dataset = TensorDataset(Tensor(x_train), Tensor(y_train))\n",
    "#test_dataset = TensorDataset(Tensor(x_test.toarray()), Tensor(y_test.toarray()))\n",
    "#train_loader = DataLoader(train_dataset, shuffle=True, batch_size=128)\n",
    "#test_loader = DataLoader(test_dataset, shuffle=True, batch_size=128)\n",
    "\n",
    "#cnnModule = CnnModule(loss=classification_loss, metric=Accuracy, verbose=True)\n",
    "#cnnModule.fit(n_output_node=y_encoder.n_classes,\n",
    "#             input_shape=(-1, x_train.shape[3], x_train.shape[1], x_train.shape[2]),\n",
    "#             train_data=train_loader,\n",
    "#             test_data=test_loader,\n",
    "#             time_limit=20*60)\n",
    "clf = ak.ImageClassifier(max_trials=3)\n",
    "clf.fit(x_train, y_train, epochs=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5. Visualizing the best CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.iter\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_1\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.beta_2\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.decay\n",
      "WARNING:tensorflow:Unresolved object in checkpoint: (root).optimizer.learning_rate\n",
      "WARNING:tensorflow:A checkpoint was restored (e.g. tf.train.Checkpoint.restore or tf.keras.Model.load_weights) but not all checkpointed values were used. See above for specific issues. Use expect_partial() on the load status object, e.g. tf.train.Checkpoint.restore(...).expect_partial(), to silence these warnings, or use assert_consumed() to make the check explicit. See https://www.tensorflow.org/guide/checkpoint#loading_mechanics for details.\n",
      "Train for 1250 steps, validate for 313 steps\n",
      "1250/1250 [==============================] - ETA: 17:26 - loss: 4.6408 - accuracy: 0.0000e+0 - ETA: 2:02 - loss: 4.7838 - accuracy: 0.0208    - ETA: 1:08 - loss: 4.7069 - accuracy: 0.01 - ETA: 46s - loss: 4.6491 - accuracy: 0.0132 - ETA: 36s - loss: 4.5999 - accuracy: 0.018 - ETA: 30s - loss: 4.5680 - accuracy: 0.022 - ETA: 26s - loss: 4.5343 - accuracy: 0.029 - ETA: 22s - loss: 4.5126 - accuracy: 0.031 - ETA: 20s - loss: 4.4668 - accuracy: 0.037 - ETA: 18s - loss: 4.4337 - accuracy: 0.040 - ETA: 17s - loss: 4.4006 - accuracy: 0.045 - ETA: 16s - loss: 4.3729 - accuracy: 0.049 - ETA: 15s - loss: 4.3413 - accuracy: 0.053 - ETA: 14s - loss: 4.3164 - accuracy: 0.056 - ETA: 13s - loss: 4.2961 - accuracy: 0.057 - ETA: 12s - loss: 4.2749 - accuracy: 0.061 - ETA: 12s - loss: 4.2529 - accuracy: 0.065 - ETA: 11s - loss: 4.2322 - accuracy: 0.068 - ETA: 11s - loss: 4.2150 - accuracy: 0.068 - ETA: 10s - loss: 4.2009 - accuracy: 0.071 - ETA: 10s - loss: 4.1765 - accuracy: 0.075 - ETA: 9s - loss: 4.1642 - accuracy: 0.077 - ETA: 9s - loss: 4.1496 - accuracy: 0.07 - ETA: 9s - loss: 4.1333 - accuracy: 0.08 - ETA: 9s - loss: 4.1157 - accuracy: 0.08 - ETA: 8s - loss: 4.1005 - accuracy: 0.08 - ETA: 8s - loss: 4.0842 - accuracy: 0.08 - ETA: 8s - loss: 4.0688 - accuracy: 0.09 - ETA: 8s - loss: 4.0555 - accuracy: 0.09 - ETA: 8s - loss: 4.0432 - accuracy: 0.09 - ETA: 7s - loss: 4.0308 - accuracy: 0.09 - ETA: 7s - loss: 4.0194 - accuracy: 0.10 - ETA: 7s - loss: 4.0112 - accuracy: 0.10 - ETA: 7s - loss: 3.9998 - accuracy: 0.10 - ETA: 7s - loss: 3.9849 - accuracy: 0.10 - ETA: 7s - loss: 3.9784 - accuracy: 0.10 - ETA: 6s - loss: 3.9677 - accuracy: 0.10 - ETA: 6s - loss: 3.9578 - accuracy: 0.11 - ETA: 6s - loss: 3.9493 - accuracy: 0.11 - ETA: 6s - loss: 3.9396 - accuracy: 0.11 - ETA: 6s - loss: 3.9345 - accuracy: 0.11 - ETA: 6s - loss: 3.9208 - accuracy: 0.11 - ETA: 6s - loss: 3.9115 - accuracy: 0.12 - ETA: 6s - loss: 3.9023 - accuracy: 0.12 - ETA: 5s - loss: 3.8948 - accuracy: 0.12 - ETA: 5s - loss: 3.8850 - accuracy: 0.12 - ETA: 5s - loss: 3.8778 - accuracy: 0.12 - ETA: 5s - loss: 3.8684 - accuracy: 0.12 - ETA: 5s - loss: 3.8643 - accuracy: 0.12 - ETA: 5s - loss: 3.8552 - accuracy: 0.12 - ETA: 5s - loss: 3.8514 - accuracy: 0.13 - ETA: 5s - loss: 3.8465 - accuracy: 0.13 - ETA: 5s - loss: 3.8387 - accuracy: 0.13 - ETA: 5s - loss: 3.8317 - accuracy: 0.13 - ETA: 4s - loss: 3.8246 - accuracy: 0.13 - ETA: 4s - loss: 3.8179 - accuracy: 0.13 - ETA: 4s - loss: 3.8074 - accuracy: 0.13 - ETA: 4s - loss: 3.8020 - accuracy: 0.13 - ETA: 4s - loss: 3.7936 - accuracy: 0.13 - ETA: 4s - loss: 3.7880 - accuracy: 0.13 - ETA: 4s - loss: 3.7817 - accuracy: 0.14 - ETA: 4s - loss: 3.7739 - accuracy: 0.14 - ETA: 4s - loss: 3.7716 - accuracy: 0.14 - ETA: 4s - loss: 3.7646 - accuracy: 0.14 - ETA: 4s - loss: 3.7569 - accuracy: 0.14 - ETA: 3s - loss: 3.7505 - accuracy: 0.14 - ETA: 3s - loss: 3.7423 - accuracy: 0.14 - ETA: 3s - loss: 3.7372 - accuracy: 0.14 - ETA: 3s - loss: 3.7301 - accuracy: 0.14 - ETA: 3s - loss: 3.7251 - accuracy: 0.15 - ETA: 3s - loss: 3.7192 - accuracy: 0.15 - ETA: 3s - loss: 3.7138 - accuracy: 0.15 - ETA: 3s - loss: 3.7095 - accuracy: 0.15 - ETA: 3s - loss: 3.7047 - accuracy: 0.15 - ETA: 3s - loss: 3.7017 - accuracy: 0.15 - ETA: 3s - loss: 3.6967 - accuracy: 0.15 - ETA: 3s - loss: 3.6926 - accuracy: 0.15 - ETA: 2s - loss: 3.6887 - accuracy: 0.15 - ETA: 2s - loss: 3.6816 - accuracy: 0.15 - ETA: 2s - loss: 3.6775 - accuracy: 0.15 - ETA: 2s - loss: 3.6705 - accuracy: 0.16 - ETA: 2s - loss: 3.6651 - accuracy: 0.16 - ETA: 2s - loss: 3.6584 - accuracy: 0.16 - ETA: 2s - loss: 3.6514 - accuracy: 0.16 - ETA: 2s - loss: 3.6448 - accuracy: 0.16 - ETA: 2s - loss: 3.6391 - accuracy: 0.16 - ETA: 2s - loss: 3.6314 - accuracy: 0.16 - ETA: 2s - loss: 3.6255 - accuracy: 0.16 - ETA: 2s - loss: 3.6195 - accuracy: 0.16 - ETA: 2s - loss: 3.6154 - accuracy: 0.17 - ETA: 2s - loss: 3.6120 - accuracy: 0.17 - ETA: 1s - loss: 3.6064 - accuracy: 0.17 - ETA: 1s - loss: 3.5991 - accuracy: 0.17 - ETA: 1s - loss: 3.5940 - accuracy: 0.17 - ETA: 1s - loss: 3.5898 - accuracy: 0.17 - ETA: 1s - loss: 3.5860 - accuracy: 0.17 - ETA: 1s - loss: 3.5822 - accuracy: 0.17 - ETA: 1s - loss: 3.5752 - accuracy: 0.17 - ETA: 1s - loss: 3.5696 - accuracy: 0.17 - ETA: 1s - loss: 3.5649 - accuracy: 0.17 - ETA: 1s - loss: 3.5611 - accuracy: 0.17 - ETA: 1s - loss: 3.5577 - accuracy: 0.17 - ETA: 1s - loss: 3.5527 - accuracy: 0.18 - ETA: 1s - loss: 3.5507 - accuracy: 0.18 - ETA: 1s - loss: 3.5482 - accuracy: 0.18 - ETA: 1s - loss: 3.5443 - accuracy: 0.18 - ETA: 0s - loss: 3.5394 - accuracy: 0.18 - ETA: 0s - loss: 3.5352 - accuracy: 0.18 - ETA: 0s - loss: 3.5323 - accuracy: 0.18 - ETA: 0s - loss: 3.5276 - accuracy: 0.18 - ETA: 0s - loss: 3.5231 - accuracy: 0.18 - ETA: 0s - loss: 3.5185 - accuracy: 0.18 - ETA: 0s - loss: 3.5145 - accuracy: 0.18 - ETA: 0s - loss: 3.5117 - accuracy: 0.18 - ETA: 0s - loss: 3.5085 - accuracy: 0.18 - ETA: 0s - loss: 3.5042 - accuracy: 0.18 - ETA: 0s - loss: 3.5009 - accuracy: 0.19 - ETA: 0s - loss: 3.4971 - accuracy: 0.19 - ETA: 0s - loss: 3.4938 - accuracy: 0.19 - ETA: 0s - loss: 3.4910 - accuracy: 0.19 - ETA: 0s - loss: 3.4892 - accuracy: 0.19 - ETA: 0s - loss: 3.4868 - accuracy: 0.19 - ETA: 0s - loss: 3.4844 - accuracy: 0.19 - 9s 7ms/step - loss: 3.4824 - accuracy: 0.1933 - val_loss: 2.9517 - val_accuracy: 0.2949\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial complete</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#4527A0\"><h1 style=\"font-size:18px\">Trial summary</h1></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Trial ID: 8da50cd6c7f159cf8e171a5668dad331</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Score: 2.9517128825568544</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-Best step: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:#7E57C2\"><h2 style=\"font-size:16px\">Hyperparameters:</h2></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-classification_head_1/dropout_rate: 0.5</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-classification_head_1/spatial_reduction_1/reduction_type: flatten</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-classification_head_2/dropout_rate: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-classification_head_2/spatial_reduction_1/reduction_type: global_avg</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-dense_block_1/dropout_rate: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-dense_block_1/num_layers: 1</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-dense_block_1/units_0: 128</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-dense_block_1/use_batchnorm: False</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_1/augment: False</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_1/block_type: vanilla</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_1/conv_block_1/dropout_rate: 0.25</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_1/conv_block_1/filters_0_0: 32</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_1/conv_block_1/filters_0_1: 64</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_1/conv_block_1/kernel_size: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_1/conv_block_1/max_pooling: True</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_1/conv_block_1/num_blocks: 1</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_1/conv_block_1/num_layers: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_1/conv_block_1/separable: False</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_1/normalize: True</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_2/augment: False</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_2/block_type: vanilla</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_2/conv_block_1/dropout_rate: 0</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_2/conv_block_1/filters_0_0: 32</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_2/conv_block_1/filters_0_1: 32</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_2/conv_block_1/filters_1_0: 32</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_2/conv_block_1/filters_1_1: 32</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_2/conv_block_1/kernel_size: 3</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_2/conv_block_1/max_pooling: True</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_2/conv_block_1/num_blocks: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_2/conv_block_1/num_layers: 2</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-image_block_2/conv_block_1/separable: False</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:blue\"> |-image_block_2/normalize: False</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<span style=\"color:cyan\"> |-optimizer: adam</span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Oracle triggered exit\n",
      "Train for 1563 steps, validate for 313 steps\n",
      "1563/1563 [==============================] - ETA: 34:12 - loss: 4.5870 - accuracy: 0.031 - ETA: 3:31 - loss: 4.8012 - accuracy: 0.025 - ETA: 1:49 - loss: 4.6762 - accuracy: 0.01 - ETA: 1:15 - loss: 4.6191 - accuracy: 0.01 - ETA: 59s - loss: 4.5810 - accuracy: 0.0248 - ETA: 48s - loss: 4.5495 - accuracy: 0.031 - ETA: 41s - loss: 4.5288 - accuracy: 0.034 - ETA: 36s - loss: 4.4964 - accuracy: 0.040 - ETA: 33s - loss: 4.4550 - accuracy: 0.045 - ETA: 30s - loss: 4.4283 - accuracy: 0.051 - ETA: 27s - loss: 4.4022 - accuracy: 0.052 - ETA: 25s - loss: 4.3753 - accuracy: 0.055 - ETA: 24s - loss: 4.3616 - accuracy: 0.054 - ETA: 22s - loss: 4.3416 - accuracy: 0.057 - ETA: 21s - loss: 4.3214 - accuracy: 0.060 - ETA: 20s - loss: 4.3022 - accuracy: 0.061 - ETA: 19s - loss: 4.2798 - accuracy: 0.064 - ETA: 19s - loss: 4.2614 - accuracy: 0.066 - ETA: 18s - loss: 4.2412 - accuracy: 0.069 - ETA: 17s - loss: 4.2262 - accuracy: 0.070 - ETA: 16s - loss: 4.2040 - accuracy: 0.075 - ETA: 16s - loss: 4.1854 - accuracy: 0.079 - ETA: 15s - loss: 4.1716 - accuracy: 0.082 - ETA: 15s - loss: 4.1536 - accuracy: 0.084 - ETA: 14s - loss: 4.1358 - accuracy: 0.087 - ETA: 14s - loss: 4.1220 - accuracy: 0.090 - ETA: 13s - loss: 4.1086 - accuracy: 0.092 - ETA: 13s - loss: 4.0950 - accuracy: 0.094 - ETA: 13s - loss: 4.0825 - accuracy: 0.095 - ETA: 12s - loss: 4.0713 - accuracy: 0.097 - ETA: 12s - loss: 4.0575 - accuracy: 0.099 - ETA: 12s - loss: 4.0443 - accuracy: 0.101 - ETA: 12s - loss: 4.0333 - accuracy: 0.102 - ETA: 11s - loss: 4.0193 - accuracy: 0.104 - ETA: 11s - loss: 4.0049 - accuracy: 0.105 - ETA: 11s - loss: 3.9980 - accuracy: 0.107 - ETA: 10s - loss: 3.9879 - accuracy: 0.109 - ETA: 10s - loss: 3.9756 - accuracy: 0.111 - ETA: 10s - loss: 3.9645 - accuracy: 0.112 - ETA: 10s - loss: 3.9556 - accuracy: 0.114 - ETA: 9s - loss: 3.9434 - accuracy: 0.116 - ETA: 9s - loss: 3.9366 - accuracy: 0.11 - ETA: 9s - loss: 3.9313 - accuracy: 0.11 - ETA: 9s - loss: 3.9225 - accuracy: 0.12 - ETA: 9s - loss: 3.9150 - accuracy: 0.12 - ETA: 9s - loss: 3.9060 - accuracy: 0.12 - ETA: 8s - loss: 3.9007 - accuracy: 0.12 - ETA: 8s - loss: 3.8936 - accuracy: 0.12 - ETA: 8s - loss: 3.8867 - accuracy: 0.12 - ETA: 8s - loss: 3.8808 - accuracy: 0.12 - ETA: 8s - loss: 3.8739 - accuracy: 0.12 - ETA: 8s - loss: 3.8696 - accuracy: 0.12 - ETA: 8s - loss: 3.8614 - accuracy: 0.12 - ETA: 7s - loss: 3.8534 - accuracy: 0.13 - ETA: 7s - loss: 3.8459 - accuracy: 0.13 - ETA: 7s - loss: 3.8353 - accuracy: 0.13 - ETA: 7s - loss: 3.8276 - accuracy: 0.13 - ETA: 7s - loss: 3.8215 - accuracy: 0.13 - ETA: 7s - loss: 3.8161 - accuracy: 0.13 - ETA: 7s - loss: 3.8090 - accuracy: 0.13 - ETA: 7s - loss: 3.8029 - accuracy: 0.13 - ETA: 6s - loss: 3.7978 - accuracy: 0.13 - ETA: 6s - loss: 3.7928 - accuracy: 0.14 - ETA: 6s - loss: 3.7847 - accuracy: 0.14 - ETA: 6s - loss: 3.7794 - accuracy: 0.14 - ETA: 6s - loss: 3.7700 - accuracy: 0.14 - ETA: 6s - loss: 3.7631 - accuracy: 0.14 - ETA: 6s - loss: 3.7565 - accuracy: 0.14 - ETA: 6s - loss: 3.7522 - accuracy: 0.14 - ETA: 6s - loss: 3.7483 - accuracy: 0.14 - ETA: 6s - loss: 3.7432 - accuracy: 0.14 - ETA: 6s - loss: 3.7392 - accuracy: 0.14 - ETA: 5s - loss: 3.7347 - accuracy: 0.15 - ETA: 5s - loss: 3.7299 - accuracy: 0.15 - ETA: 5s - loss: 3.7269 - accuracy: 0.15 - ETA: 5s - loss: 3.7233 - accuracy: 0.15 - ETA: 5s - loss: 3.7200 - accuracy: 0.15 - ETA: 5s - loss: 3.7170 - accuracy: 0.15 - ETA: 5s - loss: 3.7129 - accuracy: 0.15 - ETA: 5s - loss: 3.7071 - accuracy: 0.15 - ETA: 5s - loss: 3.7014 - accuracy: 0.15 - ETA: 5s - loss: 3.6960 - accuracy: 0.15 - ETA: 5s - loss: 3.6910 - accuracy: 0.15 - ETA: 5s - loss: 3.6859 - accuracy: 0.15 - ETA: 5s - loss: 3.6798 - accuracy: 0.16 - ETA: 4s - loss: 3.6739 - accuracy: 0.16 - ETA: 4s - loss: 3.6679 - accuracy: 0.16 - ETA: 4s - loss: 3.6635 - accuracy: 0.16 - ETA: 4s - loss: 3.6576 - accuracy: 0.16 - ETA: 4s - loss: 3.6507 - accuracy: 0.16 - ETA: 4s - loss: 3.6459 - accuracy: 0.16 - ETA: 4s - loss: 3.6425 - accuracy: 0.16 - ETA: 4s - loss: 3.6393 - accuracy: 0.16 - ETA: 4s - loss: 3.6343 - accuracy: 0.16 - ETA: 4s - loss: 3.6294 - accuracy: 0.16 - ETA: 4s - loss: 3.6250 - accuracy: 0.16 - ETA: 4s - loss: 3.6218 - accuracy: 0.17 - ETA: 4s - loss: 3.6166 - accuracy: 0.17 - ETA: 3s - loss: 3.6131 - accuracy: 0.17 - ETA: 3s - loss: 3.6087 - accuracy: 0.17 - ETA: 3s - loss: 3.6027 - accuracy: 0.17 - ETA: 3s - loss: 3.5991 - accuracy: 0.17 - ETA: 3s - loss: 3.5947 - accuracy: 0.17 - ETA: 3s - loss: 3.5921 - accuracy: 0.17 - ETA: 3s - loss: 3.5882 - accuracy: 0.17 - ETA: 3s - loss: 3.5859 - accuracy: 0.17 - ETA: 3s - loss: 3.5819 - accuracy: 0.17 - ETA: 3s - loss: 3.5792 - accuracy: 0.17 - ETA: 3s - loss: 3.5767 - accuracy: 0.17 - ETA: 3s - loss: 3.5719 - accuracy: 0.18 - ETA: 3s - loss: 3.5674 - accuracy: 0.18 - ETA: 3s - loss: 3.5644 - accuracy: 0.18 - ETA: 3s - loss: 3.5624 - accuracy: 0.18 - ETA: 2s - loss: 3.5586 - accuracy: 0.18 - ETA: 2s - loss: 3.5552 - accuracy: 0.18 - ETA: 2s - loss: 3.5506 - accuracy: 0.18 - ETA: 2s - loss: 3.5471 - accuracy: 0.18 - ETA: 2s - loss: 3.5434 - accuracy: 0.18 - ETA: 2s - loss: 3.5396 - accuracy: 0.18 - ETA: 2s - loss: 3.5358 - accuracy: 0.18 - ETA: 2s - loss: 3.5318 - accuracy: 0.18 - ETA: 2s - loss: 3.5286 - accuracy: 0.18 - ETA: 2s - loss: 3.5253 - accuracy: 0.18 - ETA: 2s - loss: 3.5235 - accuracy: 0.18 - ETA: 2s - loss: 3.5195 - accuracy: 0.18 - ETA: 2s - loss: 3.5177 - accuracy: 0.18 - ETA: 2s - loss: 3.5158 - accuracy: 0.18 - ETA: 2s - loss: 3.5130 - accuracy: 0.19 - ETA: 2s - loss: 3.5113 - accuracy: 0.19 - ETA: 1s - loss: 3.5083 - accuracy: 0.19 - ETA: 1s - loss: 3.5057 - accuracy: 0.19 - ETA: 1s - loss: 3.5023 - accuracy: 0.19 - ETA: 1s - loss: 3.4990 - accuracy: 0.19 - ETA: 1s - loss: 3.4958 - accuracy: 0.19 - ETA: 1s - loss: 3.4926 - accuracy: 0.19 - ETA: 1s - loss: 3.4894 - accuracy: 0.19 - ETA: 1s - loss: 3.4866 - accuracy: 0.19 - ETA: 1s - loss: 3.4840 - accuracy: 0.19 - ETA: 1s - loss: 3.4803 - accuracy: 0.19 - ETA: 1s - loss: 3.4781 - accuracy: 0.19 - ETA: 1s - loss: 3.4757 - accuracy: 0.19 - ETA: 1s - loss: 3.4725 - accuracy: 0.19 - ETA: 1s - loss: 3.4690 - accuracy: 0.19 - ETA: 0s - loss: 3.4655 - accuracy: 0.19 - ETA: 0s - loss: 3.4631 - accuracy: 0.19 - ETA: 0s - loss: 3.4604 - accuracy: 0.19 - ETA: 0s - loss: 3.4583 - accuracy: 0.19 - ETA: 0s - loss: 3.4554 - accuracy: 0.20 - ETA: 0s - loss: 3.4528 - accuracy: 0.20 - ETA: 0s - loss: 3.4498 - accuracy: 0.20 - ETA: 0s - loss: 3.4477 - accuracy: 0.20 - ETA: 0s - loss: 3.4451 - accuracy: 0.20 - ETA: 0s - loss: 3.4421 - accuracy: 0.20 - ETA: 0s - loss: 3.4393 - accuracy: 0.20 - ETA: 0s - loss: 3.4367 - accuracy: 0.20 - ETA: 0s - loss: 3.4341 - accuracy: 0.20 - ETA: 0s - loss: 3.4322 - accuracy: 0.20 - ETA: 0s - loss: 3.4278 - accuracy: 0.20 - ETA: 0s - loss: 3.4251 - accuracy: 0.20 - 11s 7ms/step - loss: 3.4247 - accuracy: 0.2060 - val_loss: 2.4800 - val_accuracy: 0.4086\n",
      "INFO:tensorflow:Assets written to: C:/DEV/assets\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from graphviz import Digraph\n",
    "# pip install graphviz\n",
    "from keras import *\n",
    "from autokeras.utils import *\n",
    "from keras.datasets import fashion_mnist\n",
    "\n",
    "\n",
    "clf = ak.ImageClassifier(max_trials=1)\n",
    "clf.fit(x_train, y_train, epochs=1)\n",
    "\n",
    "temp = \"C:/DEV/\"\n",
    "\n",
    "model = clf.export_model()\n",
    "tf.keras.models.save_model(model, temp)\n",
    "\n",
    "##load_model(path, custom_objects=ak.CUSTOM_OBJECTS)\n",
    "#clf.export_keras_model(MODEL_DIR)\n",
    "#clf.save('model.h', save_format='tf')\n",
    "#export_model(clf)\n",
    "#clf.export_autokeras_model('automodel.h5')\n",
    "\n",
    "\n",
    "#print(type(model))  # <class 'tensorflow.python.keras.engine.training.Model'>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
